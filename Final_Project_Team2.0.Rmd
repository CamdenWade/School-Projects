---
title: 'STAT 400 Team 2: Final Project'
author: "Joe Redmond, Leo Soccio, Camden Wade, and Tyler Sweeney"
date: "April 12, 2024"
output: pdf_document
always_allow_html: true
---

```{r, include = FALSE}
# Setting Document Options
knitr::opts_chunk$set(
  echo = FALSE,
  warning = FALSE,
  message = FALSE,
  fig.align = "center", # Sets the alignment of figures
  cache = TRUE # remembers what the code chunk runs from last time. If chunk is edited it will re-run the chunk.
  # If we wrangle data then make a graph and change the wrangle, we need to user re-run the graph to update changes
)
```


```{r, message=FALSE}
# Cleaning the environment
rm(list = ls())

# Loading the necessary packages for this project
library(tidyverse)
library(knitr)
library(kableExtra)
library(janitor)
library(cluster)
library(factoextra)
library(dendextend)
library(dplyr)
library(leaps)
library(caTools)
library(DescTools)
library(rpart)
library(rpart.plot)
library(randomForest)
library(rvest)
library(lme4)
#library(MASS)
```

# Introduction 


## Setting the Stage
One of the most exciting parts of the Major League Baseball (MLB) season is the All-Star game that occurs in mid July every year. This is a chance for fans from all over the globe to watch their favorite players compete in a friendly yet still competitive game of baseball involving the best of the best... well for the most part. While the All-Star game is full of star talent, it may not truly be the best players, at least from a statistical standpoint. Oftentimes, fan favorite players will be selected as an All-Star, even though they may not be having the greatest statistical year. This also means that some players do not get chosen who are having good statistical years. Additionally, each team must have at least 1 representative from their respective squad, however, most of the time it is indeed that best player on the team. This has been a lingering issue that many analysts, fans, and even owners seem to disagree on. There never seems to be a consensus of the players who should and should not make the game. One thought we had was to look at a player's Wins Above Replacement (WAR) values. WAR is a statistic that essentially combines every statistic into one to evaluate a player's value to a team. If a player has a WAR greater than 0, it means their team is expected to win that many more games with said player on a the team compared to an average MLB player. If WAR is less than 0, it is the opposite. If WAR equals 0, it means there would be no difference replacing said player or not since they would be considered an average player. While WAR fluctuates in definition and computation from person to person, the differences are small and values are relatively similar.

For this project, our goal is to further investigate if WAR and/or multiple other baseball statistics that can best help us predict whether a player made the All-Star game and if it would be helpful for future predictions. We will be using a data set from the 2023 MLB regular season batting statistics to determine the following questions: "Does WAR play an important role in determining whether a player made the 2023 All-star team?" "Are there any other factors that may contribute/help predict our response?" and we will look to see if a player's division has any impact on whether or not they are an All-Star (ex. Are certain divisions more competitive than others/have more talent?). To do this we have a data set of all players with more than 100 plate appearances (to ensure an adequate sample size for each player), and we will use various machine learning algorithms to determine which specific statistics impact a players WAR value the most. By the end of this project, not only will we know if WAR is important in predicting if a player made the All-Star game or not but, we will also figure out if there are other statistics that help enhance this prediction.

While having some background about the data is helpful, it is more important that we can actually read and understand whats in the data. The original data has 105 attributes, however we will not be using all 105 of those attributes. We are going to do this analysis with a subset of the original data that contains only 25 of the attributes from the original data set. For those of you who are unaware of baseball statistics, the following is an explanation of the statistics that might appear in the subsequent analysis:

## Explaining the Data
Before we get into working with the data, we should go through a little bit of background information about it. The data used in this project is the batting (offense) statistics of players from the 2023 Major League Baseball season. The data was already pre-filtered before being downloaded to only contain batters with a big enough sample size to ensure no outlier cases. That data was downloaded from Kaggle.com, however before being uploaded to Kaggle, the data was colleted by FanGraphs.com, “a website run by Fangraphs Inc., located in Arlington, Virginia, and created and owned by David Appelman that provides statistics for every player in Major League Baseball history” (FanGraphs). The 2023 MLB regular season batting statistics was collected starting on March 30, 2023 and was finished being collected on October 1, 2023. During this time period, the data was collected from 29 MLB stadiums across the United States and one in Canada. After the data was collected from the 30 MLB stadiums, the data was organized and published at FanGraphs headquarters in Arlington, Virginia. While their website does not specifically state why they collect the data, we will take an educated guess and say that FanGraphs collects the data for the fans to use. Not only is the data used for fans to keep up to date on all current and past MLB statistics, but this data can also be used by fans to help make decisions in fantasy sports. While there are part of FanGraphs in which you need a paid membership to access, the main reason FanGraphs publishes the data is to give the fans access to the data of the game they watch. In all it appears that FanGraphs obtains their data through two different sources. According to their website, “All major league baseball data including pitch type, velocity, batted ball location, and play-by-play data provided by Sports Info Solutions”(FanGraphs Baseball | Baseball Statistics and Analysis) and “Major League and Minor League Baseball data provided by Major League Baseball” (FanGraphs Baseball | Baseball Statistics and Analysis).
While having some background about the data is helpful, it is more important that we can actually read and understand whats in the data. The original data has 105 attributes, however we will not be using all 105 of those attributes. We are going to do this analysis with a subset of the original data that contains only 25 of the attributes from the original data set. For those of you who are unaware of baseball statistics, the following is an explaination of the statistics that might appear in the subsequent analysis:

*Note: In the following explanation, "/" separates statistics of similar nature, however they are all seperate statistics*

* G: Games- The number of games the given player played in (Max of 162).

* PA: Plate Appearances- The number of times the given player goes to home plate to try and hit the ball

* AB: At Bats- The number of PA minus the number of walks and sacrifice plays

* 1B/2B/3B/HR: Singles/Doubles/Triples/Home Runs- Number of times a player gets a hit where the player gets one base/two bases/three bases/four bases respectively

* H: Hits- The number of times a player reaches base by putting the ball in play (H = 1B + 2B + 3B + HR)

* R/RBI: Runs/Runs Batted In- The number of times a player touches home plate and scores a run for their team/The number of runs a player produces as a result of them being the batter

* BB/IBB/HBP: Walk/Intentional Walk/Hit By Pitch- When a batter obtains four balls while at the plate/When a batter is sent to first intentionally by the opposing team/The number of times a given player was hit by a pitch while being the batter

* SO: Strike Outs- The number of times a player obtained three strikes while being up to bat

* SF/SH: Sacrifice Fly/Sacrifice Bunt- The number of times a player hits the ball in the air to the outfield, records an out, and has a teammate score a run on the same play/The number of times a player intentionally records an out in order to make sure a run scores for their team

* GDP: Ground into Double Play- The number of times a player hits a ground ball and the defense records two outs as a result of the ground ball

* SB/CS: Stolen Bases/Caught Stealing- The number of times a player successfully advances to the next base without the batter putting the ball into the field of play/The number of times a player unsuccessfully advances to the next base without the batter putting the ball into the field of play

* AVG: Batting Average- The percentage of times a player obtains a hit (AVG = H/AB)

* OBP: On Base Percentage- The percentage of times a player successfully reaches base (OBP = H+BB+HBP/AB+BB+HBP+SF)

* SLG: Slugging Percentage- A value that shows how much power a player has (SLG = 1B(1)+2B(2)+3B(3)+HR(4)/AB)

* OPS: On Base Plus Slugging- OPS = OBP + SLG

* WAR: Wins Above Replacement- A metric that is used to determine how worthful a player is to their team in a given year

If there are still any confusions about the statistics that will be used in the subsequent analysis you can used the following sites:

* For all statistics except for WAR (Under Offense): www.mlb.com/glossary/standard-stats

* For WAR (Under Offense): www.mlb.com/glossary/advanced-stats


 
```{r}
# Loading Data Set (CSV file from Kaggle.com, Data from Fangraphs.com)
BattingStats2023 <-
read.csv("2023 MLB Batting Statistics.csv")
```

```{r}
# Using Regular Expressions to add a division column to the data set
NLEPattern <- "PHI|ATL|NYM|MIA|WSH"
NLCPattern <- "MIL|STL|CIN|CHC|PIT"
NLWPattern <- "LAD|SFG|SDP|COL|ARI"
ALEPattern <- "BAL|TBR|TOR|BOS|NYY"
ALCPattern <- "DET|CHW|MIN|CLE|KCR"
ALWPattern <- "OAK|SEA|LAA|TEX|HOU"

SimpleBattingStats2023 <-
  BattingStats2023 %>%
  mutate(Division = ifelse(grepl(NLEPattern, Team), 1, 0)) %>%
  mutate(Division = ifelse(grepl(NLCPattern, Team), 2, Division)) %>%
  mutate(Division = ifelse(grepl(NLWPattern, Team), 3, Division)) %>%
  mutate(Division = ifelse(grepl(ALEPattern, Team), 4, Division)) %>%
  mutate(Division = ifelse(grepl(ALCPattern, Team), 5, Division)) %>%
  mutate(Division = ifelse(grepl(ALWPattern, Team), 6, Division))
```


```{r}
# Here we are selecting attributes that we believe will be the best predictors for determining all star game selections
SimpleBattingStats2023 <-
  SimpleBattingStats2023 %>%
  dplyr::select("Name", "Division", "G", "AB", "PA", "H", "X1B", "X2B", "X3B", "HR", "R", "RBI", "BB", "IBB", "SO", "HBP", "SF", "SH", "GDP", "SB", "CS", "AVG", "OBP", "SLG", "OPS", "WAR") %>%
  rename("1B" = "X1B", "2B" = "X2B", "3B" = "X3B") # Renaming a couple variable columns that R renames when loading in the CSV file
```


```{r}
# Creating a new column that tells us if a player made the All-Star game or not
SimpleBattingStats2023 <-
  SimpleBattingStats2023 %>%
  mutate(All_Star = ifelse(Name %in% c("Jonah Heim", "Yandy Díaz", "Marcus Semien", "Josh Jung", "Corey Seager", "Mike Trout", "Randy Arozarena", "Aaron Judge", "Shohei Ohtani", "Salvador Perez", "Adley Rutschman", "Vladimir Guerrero Jr.", "Whit Merrifield", "Bo Bichette", "José Ramírez", "Wander Franco", "Luis Robert Jr.", "Austin Hays", "Yordan Alvarez","Adolis García","Kyle Tucker","Julio Rodríguez","Brent Rooker","Sean Murphy","Freddie Freeman","Luis Arraez","Nolan Arenado","Orlando Arcia","Ronald Acuña Jr.","Mookie Betts","Corbin Carroll","J.D. Martinez","Will Smith","Elias Díaz","Matt Olson","Ozzie Albies","Austin Riley","Dansby Swanson","Pete Alonso","Geraldo Perdomo","Lourdes Gurriel Jr.","Nick Castellanos","Juan Soto","Jorge Soler"),1,0))
```

```{r}
# Creating a new column that tells us if a player made the All-Star game or not
SimpleBattingStats2023 <-
  SimpleBattingStats2023 %>%
  mutate(Position = ifelse(Name %in% c("Jonah Heim", "Salvador Perez", "Adley Rutschman","Sean Murphy","Will Smith","Elias Díaz"),"Catcher",ifelse(Name %in% c("Freddie Freeman", "Nathaniel Lowe", "Matt Olson", "Pete Alonso", "Brandon Belt", "Ke'Bryan Hayes", "Ryan Mountcastle", "Max Muncy", "Ji-Man Choi", "C.J. Cron", "Jake Bauers", "Bobby Bradley", "Josh Naylor", "Ryan Noda", "Edwin Encarnación"),"First Base",ifelse(Name %in% c("Marcus Semien", "Ozzie Albies", "Whit Merrifield", "DJ LeMahieu", "Jose Altuve", "Nick Madrigal", "Cavan Biggio", "Jonathan Schoop", "Nick Gonzales", "Nick Ahmed"),"Second Base",ifelse(Name %in% c("Marcus Semien", "Bobby Witt Jr.", "Eugenio Suárez", "Francisco Lindor", "Adley Rutschman", "Dansby Swanson", "Wander Franco", "J.P. Crawford", "Trea Turner", "Willy Adames", "Elvis Andrus", "Matt Duffy", "Andrelton Simmons", "Amed Rosario", "Jazz Chisholm Jr.", "Paul DeJong", "José Iglesias"),"Short Stop",ifelse(Name %in% c("Alex Bregman", "Austin Riley", "Eugenio Suárez", "Josh Donaldson", "Rafael Devers", "Jake Lamb", "Jeimer Candelario", "Jonathan India", "Matt Chapman", "Kris Bryant", "Joey Wendle", "Gio Urshela"),"Third Base",ifelse(Name %in% c(),"Left Field",ifelse(Name %in% c(),"Center Field",ifelse(Name %in% c(),"Right Field",ifelse(Name %in% c("Adley Rutschman", "Salvador Perez", "Alejandro Kirk", "J.T. Realmuto", "Gary Sánchez", "Omar Narváez", "Danny Jansen", "Keibert Ruiz", "Mitch Garver", "Tom Murphy", "Travis d'Arnaud", "Carson Kelly", "Willson Contreras", "Austin Hedges", "Francisco Mejía"),"Catcher",ifelse(Name %in% c(),"Designated Hitter",ifelse(Name %in% c("Ronald Acuña Jr.", "Kyle Schwarber", "Steven Kwan", "Julio Rodríguez", "Juan Soto", "Lane Thomas", "Brandon Nimmo", "Teoscar Hernández", "George Springer", "Vladimir Guerrero Jr.", "Nick Castellanos", "Christian Yelich", "Andrew Benintendi", "Randy Arozarena", "Bryan Reynolds", "Justin Turner", "Ha-Seong Kim", "Josh Bell", "Ezequiel Tovar", "Andrew Vaughn", "Alex Verdugo", "Bo Bichette", "Manny Machado", "Joey Gallo", "Nick Senzel", "Tommy Pham", "J.D. Martinez", "Andrew McCutchen", "Brandon Marsh", "Jordan Walker", "Manuel Margot", "Yordan Alvarez", "Joey Wiemer", "Giancarlo Stanton", "Charlie Blackmon", "Josh Naylor", "Ryan Noda", "Max Kepler", "Eloy Jiménez", "Nick Heath", "Trey Mancini", "Starling Marte", "Mitch Haniger", "Jo Adell", "Cristian Pache", "Alex Kirilloff", "Mike Tauchman", "Oscar Colás", "Will Benson", "Adolis García", "Victor Robles", "Jackie Bradley Jr.", "Jared Walsh"),"Outfielders","Utility"))))))))))))
```

```{r}
# Collecting total team WAR values from last year from fangraphs.com
# Braves: 45.4
# Dodgers: 43.3
# Orioles: 38.6
# Rays: 36.6
# Astros: 40.2
# Blue Jays: 38.6
# Phillies: 37.2
# Rangers: 35.9
# Mariners: 35.7
# Brewers: 31.7
# Twins: 36.3
# Yankees: 41.1
# Diamondbacks: 35.7
# Padres: 35.1
# Cubs: 33.5
# Reds: 30.9
# Giants: 33.2
# Mets: 34.5
# Marlins: 25.2
# Red Sox: 31.1
# Guardians: 31.9
# Tigers: 27.6
# Cardinals: 34.4
# Pirates: 28.2
# Angels: 28.7
# Nationals: 19.3
# Royals: 27.5
# White Sox: 18.9
# Rockies: 15.2
# Athletics: 21.9
# Free Agents: 2.4

data <- data.frame(
  Team = c("Braves", "Dodgers", "Orioles", "Rays", "Astros", "Blue Jays", "Phillies", "Rangers", "Mariners", "Brewers", "Twins", "Yankees", "Diamondbacks", "Padres", "Cubs", "Reds", "Giants", "Mets", "Marlins", "Red Sox", "Guardians", "Tigers", "Cardinals", "Pirates", "Angels", "Nationals", "Royals", "White Sox", "Rockies", "Athletics"),
  WAR = c(45.4, 43.3, 38.6, 36.6, 40.2, 38.6, 37.2, 35.9, 35.7, 31.7, 36.3, 41.1, 35.7, 35.1, 33.5, 30.9, 33.2, 34.5, 25.2, 31.1, 31.9, 27.6, 34.4, 28.2, 28.7, 19.3, 27.5, 18.9, 15.2, 21.9)
)
```

```{r}
# Adding a division column that tells us what division each player is in
data <- 
  data %>%
  mutate(Division = ifelse(Team %in% c("Braves","Phillies","Mets","Nationals","Marlins"), 1,
               ifelse(Team %in% c("Brewers","Cardinals","Reds","Cubs","Pirates"), 2,
               ifelse(Team %in% c("Dodgers","Giants","Padres","Rockies","Diamondbacks"), 3,
               ifelse(Team %in% c("Orioles","Rays","Blue Jays","Red Sox","Yankees"), 4,
               ifelse(Team %in% c("Tigers","White Sox","Twins","Guardians","Royals"), 5,
               ifelse(Team %in% c("Athletics","Mariners","Angels","Rangers","Astros"), 6, 0)))))))
```

```{r}
# Making data set including division average WAR
grouping_division_war <-
data %>%
  group_by(Division) %>%
  summarise(avg_div_war = mean(WAR))
```

```{r}
# Creating final data set
final_dataset <- left_join(SimpleBattingStats2023,grouping_division_war,by="Division")
```

# EDA
Here we used the head and str functions to give us an idea of the data we are working with. We can see that we are given the names of each player in the data set and all the stats that we chose to be included alongside them. We also see that most of our predictors are integers/numerical values except for name and position, which is something we may consider using for the multi-level model later on. We have a binomial response variable of All-Star which tells us if a player made the All-Star team or not which is the response we will be using to determine which attributes help us the most in determining if a player was an All-Star or not last season.
```{r}
#Looking at data we are working with
head(SimpleBattingStats2023)
str(SimpleBattingStats2023)
```
```{r}
# Creating WAR summary statistic table
SimpleBattingStats2023 %>%
  summarise(
     "Lowest WAR" = min(WAR),
     "Median WAR" = median(WAR),
     "Highest WAR" = max(WAR),
     "Average WAR" = mean(WAR),
     "SD" = sd(WAR)
     ) %>%
   kbl(caption = "Summary Statistics for Wins Above Replacement (WAR)") %>%
   kable_classic() %>%
   kable_styling(latex_options = "HOLD_position")

SimpleBattingStats2023 %>%
  ggplot(aes(x = WAR)) +
  geom_histogram() +
  xlab("Wins Above Replacement (WAR)") +
  ylab("Number of Players") +
  ggtitle("WAR Histogram")
```
After first looking at WAR as a response variable, there is a lot the chart has to offer. Starting with the summary table, we can see that the range of the WAR values is -2 to 8.3. The WAR values also has a median of about 0.9 and an average WAR of 1.28. This means that when we are testing our models later on, we can expect that a majority of our results to be within a range similar to the range the original data was in. While there might be a couple outlier values, it is reasonable to expect the majority of the predicted values to fall into the -2 to 8.3 range. We can also see from the summary table that the standard deviation of the WAR attribute is about 1.76. One of the first things we noticed is that there is a right skew in the distribution. This makes since as the players who are at All-Star levels will have higher WAR values than most players. However, since there is only a finite amount of players who make the All-Star team, it explains why the skew exists. This may already help to support our initial idea that WAR can be a good predictor for those who made the All-Star game or not.


```{r}
# Preliminary EDA to see if a few stats have any effect/trend on All-Star selections
boxplot(AVG ~ All_Star, data = SimpleBattingStats2023)
boxplot(OBP ~ All_Star, data = SimpleBattingStats2023)
boxplot(WAR ~ All_Star, data = SimpleBattingStats2023)
```
The next piece of EDA we wanted to look at was some boxplots of other statistics while faceting based on players who made the All-Star game or not. Along with WAR, we also looked at a player averages (AVG) and on-base percentage (OBP). What we found was 
there was a significant difference between players who made the All-Star game for each category. All of the statistics had higher median values for All-Stars. What was interesting, however, was that there were some outliers for non All-Stars that exceeded the median for its corresponding All-Star counter parts. This helped to show that sometimes players may make the game when they may not have the best offensive statistics and vice versa. 
```{r}
# Creating Histogram of All Star
hist(SimpleBattingStats2023$All_Star)
```
Lastly, before getting into our models, we wanted to look at how many players in our data set were selected as All-Stars or not. What we found was that only a handful (44) of offensive players that made the All-Star game compared to the entire MLB. What we have to realize is that sometimes players get hurt and need replacements for extended periods of time. We also need to realize that certain players may be considered qualified hitters or appear in the data set because they had enough at-bats in the season even though they may not have been starters of got sent down to the minor leagues. Because of this, we have to be careful when cerating these models. We may find that if a model predicts everything to be negative, the true negative rate will be high simply because so many players are in the data set that did not make the All-Star game. 

# GLMs
```{r}
# Creating our first logistic model with WAR as the only predictor
Model1 <- glm(All_Star ~ WAR, family = binomial, data = SimpleBattingStats2023)
Model1
```
After creating our first glm model with just WAR as a predictor, we can see that the coefficient of WAR is 0.9294, the Null Deviance is 290.4, the Residual Deviance is 187.2, and the AIC is 191.2. Between the coefficient of WAR being positive, a lower Residual Deviance then Null Deviance and a somewhat low AIC value, we can conclude that WAR is a significant predictor in All Star Game selection. We are still interested in seeing if other predictors can also be used. 

## Selection Processes for Best Model
* Here we are using the stepwise selection process to see what the best predictors are for All_Star based on AIC levels
```{r}
# Performing stepwise selection to determine important attributes for WAR
# Data we will consider for stepwise selection
dataForStepwise <-
  SimpleBattingStats2023 %>%
  dplyr::select("Division", "G", "AB", "PA", "H", "1B", "2B", "3B", "HR", "R", "RBI", "BB", "IBB", "SO", "HBP", "SF", "SH", "GDP", "SB", "CS", "AVG", "OBP", "SLG", "OPS", "WAR","Position")


# Save Label
pred <- SimpleBattingStats2023$All_Star

#  Full and intercept models 
fullAll_StarModel <- lm(
  formula = pred ~ .,
  data = dataForStepwise
)

interceptAll_StarModel <- lm(
  formula = pred ~ 1,
  data = dataForStepwise
)

#  Stepwise Selection 
All_StarModelSS <- step(
  object = interceptAll_StarModel, 
  scope = list(
    lower = interceptAll_StarModel,
    upper = fullAll_StarModel
  ),
  data = dataForStepwise, 
  direction = "both",
  k = log(nrow(dataForStepwise)),
  trace = 0
)
```
Since we are interested if other factors/predictors can be used to predict All Star Game selection we used a stepwise regression function to find these potential predictors. After using the stepwise selection process, we found the best predictors were WAR, IBB, RBI, G, GDP, and SH.

* Here we are seeing if the AIC levels are lower for forward selection process
```{r}
# Seeing if forward selection provides a better model in terms of AIC
reduced_mod <- glm(All_Star ~ WAR, 
                   family = binomial, 
                   data = SimpleBattingStats2023) 
full_mod <- glm(All_Star ~ ., 
            family = binomial,
            data = SimpleBattingStats2023)
 
stats::step(reduced_mod, 
            scope = list(lower=reduced_mod, upper=full_mod), 
            data = SimpleBattingStats2023, 
            direction = 'forward')
```
Besides stepwise selection we also wanted to look at forward selection. From the forward selection process, we found significant factors to be WAR, RBI, Position, IBB, GDP, SH, SLG, SB, BB, SO, OBP, 2B, R. It is important to note that both models include WAR, RBI, IBB, GDP, and SH. 

* Summary data of the stepwise selection model
```{r}
# Showing the model that resulted from the Stepwise selection process
summary(All_StarModelSS)
```

Looking at the summary from the stepwise selection model, we can see that as stated earlier, WAR, IBB, RBI, G, GDP, and SH are all significant. 

* Summary outputs from models produced from forward and stepwise selection processes respectively
```{r}
# Summary outputs from models produced from forward and stepwise selection processes
Model2 <- glm(All_Star ~ WAR + RBI + Position + IBB + GDP + SH + 
    SLG + SB + BB + SO + OBP + `2B` + R, family = binomial, data = SimpleBattingStats2023)
summary(Model2)
Model3 <- glm(All_Star ~ WAR + IBB + RBI + G + GDP + SH, family = binomial, data = SimpleBattingStats2023)
summary(Model3)
```

The summaries of both models show that there are other predictors besides WAR that are significant in predicting All Star Game selection, which answers our first research question. We were also interested in seeing if either the stepwise selection or forward selection model was better at predicting All Star Game selection. From the summary we can see that the AIC value for the forward selection model was 143.02 and the AIC value for the stepwise model was 159.79, this means that we can conclude that the forward selection model is better. 

# Multi-Level Modeling

The multi-level modeling was guided by our second research question: should the division a player is in be considered when predicting their all-star status after considering player-specific information? To explore this question, we utilized the same MLB batting statistics dataset used in the previous section. Additionally, we added team payroll data sourced from [Spotrac](https://www.spotrac.com/mlb/payroll/_/year/2023/sort/cap_maximum_space2) and averaged it by division. We also averaged WAR by division. This was in order to inspect if a player being surrounded by other great players would affect their all-star status. 

It should be noted that our choice to sort players by division instead of team was deliberate. With only about 450 players in our dataset, splitting the players up into their 30 teams would likely stretch us far too thin, as we deemed about 15 players per grouping to be too few for effective model building. After splitting into six divisions instead, we kept about 75 players in each grouping, so with these better subgroup sample sizes, we hope to build a more powerful model.

The following code is simply cleaning the data and putting it into a usable state.

```{r}
# Cleaning data set to use
MLMdata <- final_dataset %>% filter(Division!=0) # Division 0 seemed to be missing or invalid in this context
```

```{r}
# Creating data set with team's payroll
paydata <- data.frame(
  Team = c("Braves", "Dodgers", "Orioles", "Rays", "Astros", "Blue Jays", "Phillies", "Rangers", "Mariners", "Brewers", "Twins", "Yankees", "Diamondbacks", "Padres", "Cubs", "Reds", "Giants", "Mets", "Marlins", "Red Sox", "Guardians", "Tigers", "Cardinals", "Pirates", "Angels", "Nationals", "Royals", "White Sox", "Rockies", "Athletics"),
  payroll = c(206239131, 240278296, 71061047, 79354272, 237107748, 214630885, 245419295, 251332754, 127966903, 125338345, 156104540, 278651150, 119257651, 256045688, 188909358, 96577288, 187398165, 343605067, 105435809, 182926796, 91861627, 121494514, 153793028, 75695975, 230534276, 93378663, 96083853, 162863836, 171026607, 62243227)
)

paydata <- 
  paydata %>%
  mutate(Division = ifelse(Team %in% c("Braves","Phillies","Mets","Nationals","Marlins"), 1,
               ifelse(Team %in% c("Brewers","Cardinals","Reds","Cubs","Pirates"), 2,
               ifelse(Team %in% c("Dodgers","Giants","Padres","Rockies","Diamondbacks"), 3,
               ifelse(Team %in% c("Orioles","Rays","Blue Jays","Red Sox","Yankees"), 4,
               ifelse(Team %in% c("Tigers","White Sox","Twins","Guardians","Royals"), 5,
               ifelse(Team %in% c("Athletics","Mariners","Angels","Rangers","Astros"), 6, 0)))))))
```

```{r}
# Creating data set of division average payroll
paymeans<-
paydata %>%
  group_by(Division) %>%
  summarise(avg_div_pay = mean(payroll))
```

```{r}
# Merging previous two data sets to get final MLM dataset
MLMdata <- left_join(MLMdata,paymeans,by="Division")

names(MLMdata)[names(MLMdata) == "1B"] <- "B1"
names(MLMdata)[names(MLMdata) == "2B"] <- "B2"
names(MLMdata)[names(MLMdata) == "3B"] <- "B3"
MLMdata <- MLMdata %>% mutate(avg_div_paysmall = avg_div_pay/100000000)
```

## MLM EDA

A fair amount of the EDA for our multi-level model is already covered by the GLM, as the level 1 variables for this model will be variables used in the GLM. However, our level 2 variables - average division WAR and payroll, as well as how other variables differ by division, should be inspected. The following is two box plots showing how payroll and WAR are distributed across divisions to explore how our level 2 variables will differ from division to division.

```{r}
# Creating box plot of division and average division payroll
ggplot(data=paydata, aes(x=Division, y=payroll, group=Division)) +
  geom_boxplot()
# Creating box plot of division and average division war
ggplot(data=data, aes(x=Division, y=WAR, group=Division)) +
  geom_boxplot()
```

In the plot of payroll vs. division, we notice that payroll does seem to vary quite a bit by division, with divisions 2 and 5 being especially low. WAR also varies a fair amount by division, with divisions 3 and 4 being especially high. These plots suggest that there are at least some tangible effects when sorting by division, so further analysis can explore whether these effects are useful in predicting all-star status or not.

Next, we faceted a few of our most important predictors by division to see if it plays a role in affecting these variables:

```{r}
# Creating box plot to show WAR change from division to division based on All Star or not
ggplot(MLMdata, aes(x=All_Star, y=WAR, group=All_Star)) +
  geom_boxplot() +
  facet_wrap(facets = "Division")
# Creating box plot to show RBI change from division to division based on All Star or not
ggplot(MLMdata, aes(x=All_Star, y=RBI, group=All_Star)) +
  geom_boxplot() +
  facet_wrap(facets = "Division")
```

For WAR, it appears that the centers of each distribution are roughly in the same spot, with only the spread slightly changing by division. This suggests that perhaps our level 2 effects may not be as helpful as we had hoped, as it implies that WAR is not correlated with division. Additionally, RBI is a similar story. Each pair of box plots looks quite similar to every other pair, with only the variance tending to differ (with division 5 having a notably low variance). Again, this suggests that division may not be as important as we imagined for predicting all-star status.


## MLM Models

To begin, we created the most expansive model that the glmer() function would allow. This model included a subset of variables selected for by both the forward and stepwise selection methods in the GLM section, since we supposed these variables would still be of interest for the generalized linear mixed model (GLMM). We also included our level 2 variables as discussed previously: average division WAR and payroll. Adding any more variables would cause the glmer() function to fail to produce a model. This is due to the fact that overfitting became such a large issue that the model would converge into absolutely perfect predictions. Overfitting is still an issue here, as the singularity error suggests, so the rest of this section will be spent reducing our model to help remedy this issue.

```{r}
# Creating first model
GLMMmodelLarge<-glmer(All_Star ~ WAR + RBI + SH + GDP + (1 + avg_div_war + avg_div_paysmall|Division), data=MLMdata, family=binomial, control=glmerControl(optimizer="bobyqa"))
# Summary of model
summary(GLMMmodelLarge)
```

Interpreting the model at this step would be all but pointless, as overfitting is such a large issue. Before discussing what our models mean, we will first find a model that appears more statistically sound.

Our next step in analysis was to ensure our GLMM model was predicting as expected and not making blatantly incorrect claims. To do this, we created a GLM model with the same four level 1 variables and compared their coefficients to that of the GLMM model:

```{r}
# Creating GLM model to verify previous model
testingGLMforGLMM<-glm(All_Star ~ WAR + RBI + SH + GDP, data=MLMdata, family=binomial)
# Summary of model
summary(testingGLMforGLMM)
```

Fortunately, our level 1 coefficients from the GLMM are very close to the GLM coefficients. This means that our level 1 effects are acting as expected before factoring in the level 2 effects. With this ensured, we proceeded with dropping terms to create a better model. We began with a random intercepts model as a baseline comparison to check if any of our covariates were improving the model.

```{r}
# Creating random intercepts model
GLMMmodelInts<-glmer(All_Star ~ 1 + (1|Division), data=MLMdata, family=binomial, control=glmerControl(optimizer="bobyqa"))
# Summary of model
summary(GLMMmodelInts)
```

In this analysis, we will be focusing on comparing the AIC of our models in order to attempt to create the model that can predict most effectively. While the GLMM uses a Laplace estimation for the likelihood, the bias of this estimator diminishes for larger sample sizes. Ideally, 447 players would be enough to make this bias negligible, so we will be comparing AIC's with the caveat that the comparisons may not be quite perfect. Despite this issue, the overfitting issue will guide us in the same direction that AIC will throughout this analysis: dropping variables will help improve the model.

Regardless, we see that the null model's AIC is much higher than that of the full model (287.9 vs. 174.3). This is to be expected, as it means that at least some of our predictor variables are adding to the predictive power of the model by a substantial amount. However, we would still like to see if all of our variables are necessary, so our next step was to remove our random slopes (the division average WAR and payroll) and compare the AIC:

```{r}
# Creating model without random slopes
GLMMNoRandSlopes<-glmer(All_Star ~  WAR + RBI + SH + GDP + (1|Division), data=MLMdata, family=binomial, control=glmerControl(optimizer="bobyqa"))
# Summary of model
summary(GLMMNoRandSlopes)
```

This new model without random slopes shows an improved AIC over either of the previous models, at 165.9. This suggests that dropping the random slopes would be wise in creating a more parsimonious model, but before discarding both terms, we will first re-add each of the two variables (average division payroll and WAR) one at a time to see if they can produce a lower AIC than 165.9.

```{r}
# Creating model with just payroll
GLMMpayrollonly<-glmer(All_Star ~  WAR + RBI + SH + GDP + (1+avg_div_paysmall|Division), data=MLMdata, family=binomial, control=glmerControl(optimizer="bobyqa"))
# Summary of model
summary(GLMMpayrollonly)
# Creating model with just WAR
GLMMwaronly<-glmer(All_Star ~  WAR + RBI + SH + GDP + (1+avg_div_war|Division), data=MLMdata, family=binomial, control=glmerControl(optimizer="bobyqa"))
# Summary of model
summary(GLMMwaronly)
```

Both of these models have slightly worse AIC values than the model without any random slopes, and since we are still receiving the singularity error for overfitting, it would be best to drop these terms and only keep the random intercept, both for better AIC and to try to combat the overfitting. With our level 2 variables discarded, we proceeded by inspecting the level 1 variables:

```{r}
# Checking significance of fixed effect variables
drop1(GLMMNoRandSlopes, test="Chisq")
```

SH is not significant in this model ($p=0.19811$), so we will also drop it from our model. All of the rest of the covariates are significant ($p<0.05$), so they will be included in our final model.

```{r}
# Creating final GLMM model
GLMMfinal<-glmer(All_Star ~  WAR + RBI + GDP + (1|Division), data=MLMdata, family=binomial, control=glmerControl(optimizer="bobyqa"))
# Summary of model
summary(GLMMfinal)
```

With only three variables left (WAR, RBI, and GDP), we observe the singularity error is no longer produced, suggesting overfitting is no longer an alarming issue. Additionally, our AIC is further improved to the lowest we have seen in this section. It should also be noted that the variance between divisions is quite small at 0.01684, so the actual effects of division on predicting all-star status are most likely negligible. Finally, we compared this model to a GLM model with the same three covariates:

```{r}
# Creating GLM model to test final model
testingGLM2<-glm(All_Star ~ WAR + RBI + GDP, data=MLMdata, family=binomial)
# Summary of model
summary(testingGLM2)
```

This model appears to function more or less identically to the multi-level model, but with an AIC that is lower by about 2. This difference is rather small, and comparing such close AIC values when one is based on the true likelihood and the other is based on the Laplace approximation would be ill-advised, but regardless we conclude that the GLMM does not improve our predictive power over an identical GLM model. This suggests that a player's division does not play an important role in predicting their all-star status, and that a model including division as a level 2 effect is simply too complicated for its own good. To achieve more parsimony, we will draw our final conclusions from the GLM model instead.


## Random Forest EDA

The random forest algorithm is a powerful tool for both variable selection and defining variable importance. Using ensemble learning or learning from hundreds of iterations of models/decision trees, random forest is able to find the most "important" variables in either increasing the accuracy or increasing the Gini index of classifications. Because of this, we will be using every predictor variable (excluding the name of the player) to train our model.


One of the most effective way in visualizing the results of a single decision tree in random forest is showing the splitting rules for a specific tree.
```{r}
# Creating initial tree
names(SimpleBattingStats2023)[names(SimpleBattingStats2023) == "1B"] <- "B1"
names(SimpleBattingStats2023)[names(SimpleBattingStats2023) == "2B"] <- "B2"
names(SimpleBattingStats2023)[names(SimpleBattingStats2023) == "3B"] <- "B3"

initialallstarTree<-rpart(
  formula=All_Star~WAR + RBI +IBB + GDP + SH + 
    SLG + SB + BB + SO + OBP + B2 + R,
  data=SimpleBattingStats2023,
  method="class",
  parms = list(split="information")
)

rpart.plot(
  x=initialallstarTree,
  type=1,
  extra=101,
  varlen= 0,
  cex=0.8
)
```

Using the features from the final GLM model (WAR, RBI, Position , IBB, GDP, SH , SLG, SB, BB ,SO, OBP, 2B , R) the decision tree gives us several splitting decisions and five terminal nodes. The decisions for splitting involve the number of RBI, SLG, SO, and IBB. These splits provides us with terminal nodes that are relatively homogeneous. The majority of players within our data set have less than 65 RBI, classifying these players as non all-stars. The decision tree classifies players with over 65 RBI and a slugging percentage greater than 0.5 and players with greater than 65 RBI, less than a slugging percentage of 0.5, less than 104 strikeouts and more than 2 IBB as all stars. All other nodes are classified as non all-stars. The false negative rate, particularly for players with a slugging greater than 0.5 and more than 104 strikeout is relatively high. As such, we will use the random forest algorithm to find features within our dataset that provide the best predictive performance/best split in determining if a player was an all-star.


Because random forest takes on random iterations of models as well as random samples of a select number of predictors, a randomly generated seed will be used for the creation and tuning of the model. For our model creation, we will be using a 80/20 training validation split in order to test our model to new data in order to check for overfitting. In general however, there are several features of random forest which account for overfitting which will be discussed later.
```{r}
# Creating 80/20 training validation split

set.seed(1234)
trainInd<- sample(1:nrow(SimpleBattingStats2023), floor(0.8*nrow(SimpleBattingStats2023)))



Train<- SimpleBattingStats2023[trainInd,]
Validation<- SimpleBattingStats2023[-trainInd,]


set.seed(NULL)
```



In the process of creating a random forest model, it is essential that various parameters are tested in order to properly tune the model. The three main parameters of interest are ntrees (number of trees to grow/iterations to take place), mtry (the number of variables that will be sampled at each split in the decision tree) and nodeSize (the minimum size of terminal nodes for each decision tree). For tuning the model, ntrees will be held constant with 1000 trees chosen arbitrarily. The value for ntrees is rather insignificant on the effect of the model as long as it is sufficiently large. Anything over 500 trees should be enough in our case to test every predictor variable a few times (one of the internal checks for overfitting). The parameters of mtry and nodeSize will need to tuned however.

```{r}
# Testing various parameters to tune model
set.seed(1234)

mtry<-1:25
nodesize<- 1:30
accdf<-data.frame(replicate(max(nodesize), rep(NA,max(mtry))))

for(i in min(mtry):max(mtry)){
  for(j in min(nodesize):max(nodesize)){
  rfModel <- randomForest(as.factor(All_Star)~.-Name, data = Train, ntree=1000, mtry=i, nodesize=j)
  predRes <- predict(rfModel, newdata=Validation, type="response")
  accdf[i,j]<-mean(predRes==Validation$All_Star)
  }
}

set.seed(NULL)
```
```{r}
tempdf<-accdf|>
  pivot_longer(cols=c("X1","X2","X3","X4","X5","X6","X7","X8","X9","X10","X11","X12","X13","X14","X15","X16","X17","X18","X19","X20","X21","X22","X23","X24","X25","X26","X27","X28","X29","X30"), names_to = "NodeSize")

tempdf$NodeSize<- gsub("^X","", tempdf$NodeSize)

tempdf$NodeSize<- factor(tempdf$NodeSize, levels=c("1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17","18","19","20","21","22","23","24","25","26","27","28","29","30"))

tempdf<-tempdf|>
  mutate(mtry = rep(1:(n() %/% 30 + 1), each = 30, length.out = n()))

tempdf<-tempdf|>
  group_by(mtry)|>
  mutate(AvgAccP=mean(value))


tempdf<-tempdf|>
  group_by(NodeSize)|>
  mutate(AvgAccN=mean(value))



tempdf|>
  ggplot(aes(x=mtry,y=AvgAccP))+
  geom_point()+
  geom_smooth()

tempdf|>
  ggplot(aes(x=as.numeric(NodeSize), y=AvgAccN))+
  geom_point()+
  geom_smooth()
```
 
 This process done with nested for loops is computationally inefficient and doesn't give us the exact values for the tuned parameters we are looking for. The function tuneRF can be used to assist with the process in a more computationally efficient manner. A general rule in tuning is to use the square root of the number of predictors as the value of mtry, or the number of variables randomly sampled at each split. As such 5 will be used as the starting point for mtry and will be inflated or deflated in order to find improvements in the OOB (out of bag) error of the model.
```{r}
# Tuning continued
set.seed(1234)
tuneRF(x=SimpleBattingStats2023[,c(2:26,28)], y=SimpleBattingStats2023$All_Star, ntreeTry = 1000, mtryStart = 5, stepFactor = 2, plot=TRUE)
set.seed(NULL)
```
 
 From the tuning of the random forest model with 1000 trees, the value of mtry equals 10 was found to produce the biggest and most significant decrease in out of bag error. As such a value of 10 will be used in the creation of the model.
 
```{r}
# Creating model
set.seed(1234)
rfModel<- randomForest(as.factor(All_Star)~.-Name, data=Train, ntree=1000, mtry=10, importance=TRUE)

set.seed(NULL)

```


## Random Forest Analysis

After creating the model with 10 variables randomly sampled at each split we test the model on our validation data.
```{r}
# Testing model on our validation data
predRes<- predict(rfModel, newdata=Validation, type="response")

table(Validation$All_Star,predRes)

```

The resulting confusion matrix expresses that our model has an accuracy of 0.0.892 but a sensitivity of 0.25. This may reflect overfitting in the model. 

```{r}
# Analyzing of predictors
rfModel$importance
```

Looking at the coefficients of the predictors in classifying an observation provides little information, however looking at the MeanDecreaseAccuracy and MeanDecreaseGini of the predictor expresses the significance of each predictor in the iterations of the model.


```{r}
# Analyzing MeanDecreaseAccuracy & MeanDecreaseGini
varImpPlot(rfModel, n.var=5)
```

The MeanDecreaseAccuracy represents the ability of each predictor to generalize to new data. The metric essentailly expresses the rate at which the out of bag error is decreased when the predictor is chosen for a split. MeanDecreaseGini expresses the ability of a predictor to impact the purity of terminal nodes. When the variable is included in a split, its Gini Index expresses the amount of "purity" or how mixed the two classes are (All Star and non-All Star) within a node after a split. Larger values for both metrics represents greater significance in improving predictive accuracy. In both metrics, the top 5 predictors WAR, RBI, IBB, HR, and H all provide the best results predictive results in the splitting of the data. This helps us answer our first research question, WAR has been found to be an effective predictor variable in classifying a player as an All Star.

# Conclusion
## What We Learned
This project has confirmed our suspicions that certain offensive metrics are better at predicting whether an offensive player made the All-Star game or not. However, through the completion of this project, we learned the statistics that are most important in determining whether a player was selected or not. After our analyses of various models and methods, we concluded that wins above replacement (WAR) is the best predictor for determining whether a player was selected to the All-Star. Aside from WAR, we  concluded that the following variables were good predictors as well: runs batted in (RBI), player's position (POSITON), intentional walks (IBB), grounded into double plays (GDP), sacrifice hits (SH), slugging percentage (SLG), stolen bases (SB), walks (BB), strike outs (SO), on base percentage (OBP), doubles (2B), and runs (R). We also found that division was not important in making these predictions using the MLMs throughout the project. Because we ran into over fitting issues, we felt that the best model was the GLM of all of the predictors listed above.

## Refinements
One thing we considered when creating these models was the statistics that we wanted to use. We chose to include the predictors we did for various reasons. One of the main ones was for simplicity. Working with running, pitching, and defensive statistics is hard to measure due to their complex nature and interpretation is just as difficult. We felt that we were more comfortable looking at solely offensive statistics, especially since we had a good working knowledge on them. We also chose to focus on them because of how the All-Star game works. Because fans vote to select them majority of the players, we had to tackle this project from their perspectives. We felt it was imperative to focus on offensive statistics because that is what the fans want to see. There's an old cliche in baseball that "chicks dig the long ball." We realized that your typical fan wants to see offensive, the big home runs and balls flying off of the bat. While stealing bases and making catches with a 5% catch probability is just as important to the game of baseball, most fans do not give it enough attention. This is why we decided to stick with offensive metrics because most fans want to see that.

If we were to do this project again, one thing we may consider doing is looking at players from the perspective of other statistics that measure stolen bases and defensive ratings. This may help to get the full picture and explain why some fans voted for a specific player, even if their offensive metrics are not the best. That way, we can get a better evaluation of a player as a whole and not only focus on an area that may not be a strong suit for some. We may also consider incorporating more advanced statistics into our models since they seem to capture more about a player than most can see with the naked eye. While they may be difficult to formulate or interpret all the time, they definitely can add more value than the statistics that have been around since the 1800s.

# Code Appendix

```{r codeAppendix}
#| ref.label = knitr::all_labels(),
#| echo = TRUE,
#| eval = FALSE
```


